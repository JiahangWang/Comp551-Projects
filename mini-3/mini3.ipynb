{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-10T08:34:39.036966Z",
     "start_time": "2023-11-10T08:34:39.020486200Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler\n",
    "from transformers import AutoModelForSequenceClassification, AdamW\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data\n",
    "\n",
    "use command line tool to preprocess the raw dataset first:\n",
    "jq -r '. | [.text, .label | tostring] | @csv' train.jsonl | awk 'BEGIN{print \"text,label\"}{print}' > train.csv\n",
    "jq -r '. | [.text, .label | tostring] | @csv' test.jsonl | awk 'BEGIN{print \"text,label\"}{print}' > test.csv"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "266beb83823e63b8"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                                text  label\n",
      "0                            i didnt feel humiliated      0\n",
      "1  i can go from feeling so hopeless to so damned...      0\n",
      "2   im grabbing a minute to post i feel greedy wrong      3\n",
      "3  i am ever feeling nostalgic about the fireplac...      2\n",
      "4                               i am feeling grouchy      3\n",
      "(16000, 2)\n",
      "\n",
      "Test Data:\n",
      "                                                text  label\n",
      "0  im feeling rather rotten so im not very ambiti...      0\n",
      "1          im updating my blog because i feel shitty      0\n",
      "2  i never make her separate from me because i do...      0\n",
      "3  i left with my bouquet of red and yellow tulip...      1\n",
      "4    i was feeling a little vain when i did this one      0\n",
      "(2000, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# Display\n",
    "print(\"Train Data:\")\n",
    "print(train_data.head())\n",
    "print(train_data.shape)\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_data.head())\n",
    "print(test_data.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:34:22.235914500Z",
     "start_time": "2023-11-10T07:34:22.109432300Z"
    }
   },
   "id": "201a0ab9b81f1e92"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vectorize text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1545924bb673e8e9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformed Train text Data:\n",
      "(16000, 16158)\n",
      "\n",
      "Transformed Test text Data:\n",
      "(2000, 16158)\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit on both datasets to ensure vocabulary is consistent across both datasets\n",
    "combined_text = pd.concat([train_data['text'], test_data['text']], axis=0)\n",
    "vectorizer.fit(combined_text)\n",
    "\n",
    "# Transform the training and testing data separately\n",
    "X_train = vectorizer.transform(train_data['text'])\n",
    "X_test = vectorizer.transform(test_data['text'])\n",
    "\n",
    "# Convert to DataFrame\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "train_df = pd.DataFrame(X_train.toarray(), columns=feature_names)\n",
    "test_df = pd.DataFrame(X_test.toarray(), columns=feature_names)\n",
    "\n",
    "# Display the transformed data\n",
    "print(\"\\nTransformed Train text Data:\")\n",
    "# print(train_df.head())\n",
    "print(train_df.shape)\n",
    "print(\"\\nTransformed Test text Data:\")\n",
    "# print(test_df.head())\n",
    "print(test_df.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:34:22.849439300Z",
     "start_time": "2023-11-10T07:34:22.187115700Z"
    }
   },
   "id": "1283f1830434b7c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## convert into ndarray"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6231e74e8fb5e5ab"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (16000, 1)\n",
      "y_test shape: (2000, 1)\n",
      "x_train shape: (16000, 16158)\n",
      "x_test shape: (2000, 16158)\n"
     ]
    }
   ],
   "source": [
    "# Extract labels and convert to ndarray\n",
    "y_train = train_data['label'].to_numpy().reshape(-1, 1)\n",
    "y_test = test_data['label'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# Convert feature DataFrame to ndarray\n",
    "x_train = train_df.to_numpy()\n",
    "x_test = test_df.to_numpy()\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:34:22.863162500Z",
     "start_time": "2023-11-10T07:34:22.851391400Z"
    }
   },
   "id": "8989e03531226c8e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Naive Bayes Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfe7c2987d46080c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_log_prior_ = None\n",
    "        self.feature_log_prob_ = None\n",
    "        self.classes_ = None\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Calculate the prior probability of each class and take the logarithm\n",
    "        self.classes_, y_counts = np.unique(y, return_counts=True)\n",
    "        self.class_log_prior_ = np.log(y_counts / y_counts.sum())\n",
    "\n",
    "        # Calculate the conditional probability P(feature|class)\n",
    "        self.feature_log_prob_ = []\n",
    "        for c in self.classes_:\n",
    "            # Select samples for each class\n",
    "            x_c = x[y == c]\n",
    "            # Calculate the probability of each feature and take the logarithm\n",
    "            class_feature_prob = (x_c.sum(axis=0) + 1) / (x_c.sum() + x_c.shape[1])\n",
    "            self.feature_log_prob_.append(np.log(class_feature_prob))\n",
    "\n",
    "        self.feature_log_prob_ = np.array(self.feature_log_prob_)\n",
    "\n",
    "    def predict(self, xt):\n",
    "        # Calculate the probability for each sample belonging to each class\n",
    "        log_probs = xt @ self.feature_log_prob_.T + self.class_log_prior_\n",
    "        # Choose the class with the highest probability\n",
    "        return self.classes_[np.argmax(log_probs, axis=1)]\n",
    "\n",
    "    def eval(self, y_true, y_pred):\n",
    "        # Calculate accuracy\n",
    "        return (y_true == y_pred).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T07:05:37.038895300Z",
     "start_time": "2023-11-09T07:05:37.027630700Z"
    }
   },
   "id": "5cf5b05671cd8770"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### evaluate model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "933ca69f728bf689"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.769\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Naive Bayes classifier\n",
    "nb = NaiveBayes()\n",
    "\n",
    "# Train the model\n",
    "nb.fit(x_train, y_train.flatten())  # Assuming y_train is a two-dimensional array\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = nb.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = nb.eval(y_test.flatten(), y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T07:05:38.351948500Z",
     "start_time": "2023-11-09T07:05:37.032911400Z"
    }
   },
   "id": "a0307222497fbf0b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.769\n",
      "Precision: 0.807\n",
      "Recall: 0.658\n",
      "F1 Score: 0.685\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(y_true, y_pred, num_classes):\n",
    "    # Initialize metrics\n",
    "    precision = np.zeros(num_classes)\n",
    "    recall = np.zeros(num_classes)\n",
    "    f1 = np.zeros(num_classes)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    for cls in range(num_classes):\n",
    "        true_positive = np.sum((y_pred == cls) & (y_true == cls))\n",
    "        false_positive = np.sum((y_pred == cls) & (y_true != cls))\n",
    "        false_negative = np.sum((y_pred != cls) & (y_true == cls))\n",
    "        true_negative = np.sum((y_pred != cls) & (y_true != cls))\n",
    "\n",
    "        precision[cls] = true_positive / (true_positive + false_positive) if true_positive + false_positive > 0 else 0\n",
    "        recall[cls] = true_positive / (true_positive + false_negative) if true_positive + false_negative > 0 else 0\n",
    "        f1[cls] = 2 * (precision[cls] * recall[cls]) / (precision[cls] + recall[cls]) if precision[cls] + recall[cls] > 0 else 0\n",
    "\n",
    "    # Calculate the macro-average of the metrics\n",
    "    macro_precision = np.mean(precision)\n",
    "    macro_recall = np.mean(recall)\n",
    "    macro_f1 = np.mean(f1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(y_true == y_pred)\n",
    "\n",
    "    return accuracy, macro_precision, macro_recall, macro_f1\n",
    "\n",
    "# Use the calculate_metrics function\n",
    "num_classes = 6  \n",
    "accuracy, precision, recall, f1 = calculate_metrics(y_test.flatten(), y_pred, num_classes)\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {accuracy:.3f}\")\n",
    "print(f\"Precision: {precision:.3f}\")\n",
    "print(f\"Recall: {recall:.3f}\")\n",
    "print(f\"F1 Score: {f1:.3f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-09T07:05:42.393268400Z",
     "start_time": "2023-11-09T07:05:42.379792900Z"
    }
   },
   "id": "1867853af2194bbb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process data for bert"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0a2fb834eecf025"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([16000, 87])\n",
      "x_test shape: torch.Size([2000, 87])\n",
      "y_train_tensor shape: torch.Size([16000, 1])\n",
      "y_test_tensor shape: torch.Size([2000, 1])\n"
     ]
    }
   ],
   "source": [
    "# 1. Load pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. Tokenizer\n",
    "def tokenize_texts(texts):\n",
    "    return tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# 3. Apply tokenization\n",
    "# train_encodings = tokenize_texts(train_data['text'].tolist())\n",
    "# test_encodings = tokenize_texts(test_data['text'].tolist())\n",
    "train_encodings = tokenizer(train_data['text'].tolist(), padding='max_length', truncation=True, max_length=87, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_data['text'].tolist(), padding='max_length', truncation=True, max_length=87, return_tensors=\"pt\")\n",
    "\n",
    "# 4. Convert tokenized data to tensors\n",
    "x_train = train_encodings['input_ids']\n",
    "x_test = test_encodings['input_ids']\n",
    "\n",
    "\n",
    "attention_masks_train = train_encodings['attention_mask']\n",
    "attention_masks_test = test_encodings['attention_mask']\n",
    "\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "y_test_tensor = torch.tensor(y_test)\n",
    "\n",
    "# Print the shape of all tensors\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
    "print(\"y_test_tensor shape:\", y_test_tensor.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T07:52:34.235873700Z",
     "start_time": "2023-11-10T07:52:31.862048Z"
    }
   },
   "id": "9b66baf70c9454f1"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 8\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# 1. 准备模型：添加一个用于分类的顶层，并移到适当的设备\u001B[39;00m\n\u001B[0;32m      7\u001B[0m model \u001B[38;5;241m=\u001B[39m AutoModelForSequenceClassification\u001B[38;5;241m.\u001B[39mfrom_pretrained(model_name, num_labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m6\u001B[39m)\n\u001B[1;32m----> 8\u001B[0m model\u001B[38;5;241m.\u001B[39mto(device)  \u001B[38;5;66;03m# 将模型移动到指定的 device 上\u001B[39;00m\n\u001B[0;32m      9\u001B[0m model\u001B[38;5;241m.\u001B[39meval()  \u001B[38;5;66;03m# 将模型设置为评估模式\u001B[39;00m\n\u001B[0;32m     11\u001B[0m \u001B[38;5;66;03m# 2. 定义数据加载器\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\transformers\\modeling_utils.py:2014\u001B[0m, in \u001B[0;36mPreTrainedModel.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   2009\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2010\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`.to` is not supported for `4-bit` or `8-bit` bitsandbytes models. Please use the model as it is, since the\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2011\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2012\u001B[0m     )\n\u001B[0;32m   2013\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 2014\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1156\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m                     non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[0;32m   1158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n\u001B[1;32m-> 1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_apply(convert)\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 810\u001B[0m         module\u001B[38;5;241m.\u001B[39m_apply(fn)\n\u001B[0;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    829\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    830\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    831\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    832\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 833\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m fn(param)\n\u001B[0;32m    834\u001B[0m should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    835\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_use_set_data:\n",
      "File \u001B[1;32mC:\\Program Files\\anaconda3\\envs\\551code\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1158\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1155\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1156\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m                 non_blocking, memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format)\n\u001B[1;32m-> 1158\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(device, dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, non_blocking)\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# 检查是否有可用的 GPU，如果有，使用它；否则使用 CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "# 1. 准备模型：添加一个用于分类的顶层，并移到适当的设备\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6)\n",
    "model.to(device)  # 将模型移动到指定的 device 上\n",
    "model.eval()  # 将模型设置为评估模式\n",
    "\n",
    "# 2. 定义数据加载器\n",
    "test_dataset = TensorDataset(x_test)  # 使用测试数据集\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# 3. 进行预测\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    # 将数据移动到相同的 device\n",
    "    b_input_ids = batch[0].to(device)\n",
    "\n",
    "    with torch.no_grad():  # 在评估模式下，不计算梯度\n",
    "        outputs = model(b_input_ids)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    predictions.append(logits)\n",
    "\n",
    "# 转换预测结果为更易理解的格式（例如，提取最可能的类别）\n",
    "predicted_labels = [torch.argmax(logits, dim=1).cpu().numpy() for logits in predictions]\n",
    "predicted_labels_flat = np.concatenate(predicted_labels, axis=0)\n",
    "\n",
    "# 确保 y_test 也是 NumPy 数组\n",
    "y_test_np = y_test.cpu().numpy() if torch.is_tensor(y_test) else y_test\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test_np, predicted_labels_flat)\n",
    "print(\"Prediction Accuracy: {:.2f}%\".format(accuracy * 100))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-10T08:34:45.327244900Z",
     "start_time": "2023-11-10T08:34:43.875725300Z"
    }
   },
   "id": "3ec80997667e05d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
